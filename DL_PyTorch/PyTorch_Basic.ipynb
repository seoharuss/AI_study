{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ESFOu0JvqH1X",
        "wapucqYfqixb",
        "mVzx0pTKUgIZ",
        "ifK01lDoXmYy",
        "TWWkJbdGWdc0",
        "L7AIKWWUcFZJ"
      ],
      "authorship_tag": "ABX9TyNm+2asGESmtkFgpdhPTkFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoharuss/AI_study/blob/main/PyTorch_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파이토치 패키지의 기본구성\n"
      ],
      "metadata": {
        "id": "Hzg2RY3iinh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. torch\n",
        "- 메인 네임스페이스\n",
        "- Numpy와 유사한 구조\n",
        "\n",
        "2. torch.autograd\n",
        "- 자동 미분을 위한 함수들 포함\n",
        "- 콘텍스트 매니저(enable_grad/no_grad)\n",
        "  - 자동 미분의 on/off 제어\n",
        "- Function\n",
        "  - 자체 미분 가능 함수를 정의할 때 사용\n",
        "\n",
        "3. torch.nn\n",
        "- 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의\n",
        "- RNN, LSTM과 같은 레이어\n",
        "- ReLU와 같은 활성화 함수\n",
        "- MSELoss와 같은 손실 함수\n",
        "\n",
        "4. torch.optim\n",
        "- 확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘 구현\n",
        "\n",
        "5. torch.utils.data\n",
        "- SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수 포함\n",
        "\n",
        "6. torch.onnx\n",
        "- ONNX(Open Neural Network Exchange)의 포맷으로 모델을 export할 때 사용\n",
        "- ONNX는 서로 다른 딥러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷"
      ],
      "metadata": {
        "id": "Q5iIRBkUj3rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치는 Numpy와 매우 유사하다.\n",
        "- 하지만 더 좋다 :)\n"
      ],
      "metadata": {
        "id": "HPIs7opJp8MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "DNOoYYPUqGLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1차원 with PyTorch\n",
        "\n",
        "1차원 텐서인 벡터 만들기"
      ],
      "metadata": {
        "id": "ESFOu0JvqH1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "nO78cgTPqPD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c4fc76-dafd-4095-ac6e-ba2dded322d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dim()을 사용하면 현재 텐서의 차원을 보여줍니다.\n",
        "\n",
        "shape나 size()를 사용하면 크기를 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "7DF_ZIddqStC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.dim())  # rank. 즉, 차원\n",
        "print(t.shape)  # shape\n",
        "print(t.size()) # shape\n",
        "\n",
        "# 1\n",
        "# torch.Size([7])\n",
        "# torch.Size([7])\n"
      ],
      "metadata": {
        "id": "OFjE4abDqUvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21a3f81-5d6e-4f71-aac8-ab555dcd9d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "torch.Size([7])\n",
            "torch.Size([7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 1차원 텐서이며, 원소는 7개입니다. 인덱스로 접근하는 것과 슬라이싱을 해봅시다."
      ],
      "metadata": {
        "id": "oYRij4P6qbPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
        "print(t[2:5], t[4:-1])    # 슬라이싱\n",
        "print(t[:2], t[3:])       # 슬라이싱\n",
        "\n",
        "# tensor(0.) tensor(1.) tensor(6.)\n",
        "# tensor([2., 3., 4.]) tensor([4., 5.])\n",
        "# tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
      ],
      "metadata": {
        "id": "1ZEmHFTEqdbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd17bab6-1b90-4502-cdb6-dc8054de377d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.) tensor(1.) tensor(6.)\n",
            "tensor([2., 3., 4.]) tensor([4., 5.])\n",
            "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2차원 with PyTorch\n",
        "\n",
        "2차원 텐서인 행렬 만들기"
      ],
      "metadata": {
        "id": "wapucqYfqixb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1., 2., 3.],\n",
        "                       [4., 5., 6.],\n",
        "                       [7., 8., 9.],\n",
        "                       [10., 11., 12.]\n",
        "                      ])\n",
        "print(t)\n",
        "\n",
        "# tensor([[ 1.,  2.,  3.],\n",
        "#         [ 4.,  5.,  6.],\n",
        "#         [ 7.,  8.,  9.],\n",
        "#         [10., 11., 12.]])\n"
      ],
      "metadata": {
        "id": "0VODaH-mqqW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85718bc6-9e14-4fbc-f0ba-ae26d1d44efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dim()을 사용하면 현재 텐서의 차원을 보여줍니다.\n",
        "\n",
        "size()를 사용하면 크기를 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "XxuiXZ_JquTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.dim())  # rank. 즉, 차원\n",
        "print(t.size()) # shape\n",
        "\n",
        "# 2\n",
        "# torch.Size([4, 3])"
      ],
      "metadata": {
        "id": "rTwLgftHqwQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd87ac5-99fa-471b-ed36-22e02afd34fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 브로드캐스팅(Broadcasting)\n",
        "\n",
        "두 행렬 A, B가 있다고 해봅시다.\n",
        "\n",
        "행렬의 덧셈과 뺄셈에 대해 알고계신다면, 이 덧셈과 뺄셈을 할 때에는 두 행렬 A, B의 크기가 같아야한다는 것을 알고계실겁니다.\n",
        "\n",
        "그리고 두 행렬이 곱셈을 할 때에는 A의 마지막 차원과 B의 첫번째 차원이 일치해야합니다.\n",
        "\n",
        "물론, 이런 규칙들이 있지만 딥 러닝을 하게되면 불가피하게 크기가 다른 행렬 또는 텐서에 대해서 사칙 연산을 수행할 필요가 있는 경우가 생깁니다.\n",
        "\n",
        "이를 위해 파이토치에서는 자동으로 크기를 맞춰서 연산을 수행하게 만드는 브로드캐스팅이라는 기능을 제공합니다."
      ],
      "metadata": {
        "id": "mVzx0pTKUgIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 벡터와 스칼라가 덧셈 연산을 수행하는 것을 보여줍니다.\n",
        "\n",
        "물론, 수학적으로는 원래 연산이 안 되는게 맞지만 파이토치에서는 브로드캐스팅을 통해 이를 연산합니다."
      ],
      "metadata": {
        "id": "bykavelMVLVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector + scalar\n",
        "m1 = torch.FloatTensor([[1, 2]])\n",
        "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
        "print(m1 + m2)\n",
        "\n",
        "# tensor([[4., 5.]])"
      ],
      "metadata": {
        "id": "42weOfFJVQQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311c5ceb-9696-4f06-8035-91f267d7b417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 벡터 간 연산에서 브로드캐스팅이 적용되는 경우를 보겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "_VqWJVDTV5ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 x 1 Vector + 1 x 2 Vector\n",
        "m1 = torch.FloatTensor([[1, 2]])\n",
        "m2 = torch.FloatTensor([[3], [4]])\n",
        "print(m1 + m2)\n",
        "\n",
        "# tensor([4., 5.],\n",
        "#        [5., 6.]])\n"
      ],
      "metadata": {
        "id": "x_RjtMMGV7zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79aba56a-1099-49c7-9c50-572a2c6442a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 5.],\n",
            "        [5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "m1의 크기는 (1, 2) m2의 크기는 (2, 1)였습니다.\n",
        "\n",
        "이 두 벡터는 원래 수학적으로는 덧셈을 수행할 수 없습니다.\n",
        "\n",
        "그러나 파이토치는 두 벡터의 크기를 (2, 2)로 변경하여 덧셈을 수행합니다."
      ],
      "metadata": {
        "id": "wBaJZYBIWBg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 브로드캐스팅 과정에서 실제로 두 텐서가 어떻게 변경되는지 보자.\n",
        "# [1, 2]\n",
        "# ==> [[1, 2],\n",
        "#      [1, 2]]\n",
        "# [3]\n",
        "# [4]\n",
        "# ==> [[3, 3],\n",
        "#      [4, 4]]"
      ],
      "metadata": {
        "id": "_u477dSlWEXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "브로드캐스팅은 편리하지만, 자동으로 실행되는 기능이므로 사용자 입장에서 굉장히 주의해서 사용해야 합니다.\n",
        "\n",
        "예를 들어 A 텐서와 B 텐서가 있을 때, 사용자는 이 두 텐서의 크기가 같다고 착각하고 덧셈 연산을 수행했다고 가정해보겠습니다.\n",
        "\n",
        "하지만 실제로 이 두 텐서의 크기는 달랐고 브로드캐스팅이 수행되어 덧셈 연산이 수행되었습니다.\n",
        "\n",
        "만약, 두 텐서의 크기가 다르다고 에러를 발생시킨다면 사용자는 이 연산이 잘못되었음을 바로 알 수 있지만 브로드캐스팅은 자동으로 수행되므로 사용자는 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있습니다."
      ],
      "metadata": {
        "id": "caqWzLA7WGrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자주 사용되는 기능들\n"
      ],
      "metadata": {
        "id": "96ihdkWJXi-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 행렬 곱셈과 곱셈의 차이\n",
        "\n",
        "> Matrix Multiplication vs. Multiplication\n",
        "\n",
        "행렬로 곱셈을 하는 방법은 크게 2가지가 있다\n",
        "1. 행렬 곱셈 (`.matmul`)\n",
        "2. 원소 별 곱셈 (`.mul`)"
      ],
      "metadata": {
        "id": "ifK01lDoXmYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "m2 = torch.FloatTensor([[1], [2]])\n",
        "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
        "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
        "print(m1.matmul(m2)) # 2 x 1\n",
        "\n",
        "# Shape of Matrix 1:  torch.Size([2, 2])\n",
        "# Shape of Matrix 2:  torch.Size([2, 1])\n",
        "# tensor([[ 5.],\n",
        "#         [11.]])"
      ],
      "metadata": {
        "id": "_DaBiVOcX8EA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853ab161-992a-43de-fc74-288392599ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Matrix 1:  torch.Size([2, 2])\n",
            "Shape of Matrix 2:  torch.Size([2, 1])\n",
            "tensor([[ 5.],\n",
            "        [11.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 element-wise 곱셈이라는 것이 존재한다.\n",
        "\n",
        "동일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱하는 것을 말한다.\n",
        "\n",
        "아래는 서로 다른 크기의 행렬이 브로드캐스팅이 된 후에 element-wise 곱셈이 수행되는 것을 보여준다.\n",
        "\n",
        "이는 * 또는 mul()을 통해 수행한다."
      ],
      "metadata": {
        "id": "QSL0i3u6aUgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "m2 = torch.FloatTensor([[1], [2]])\n",
        "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
        "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
        "print(m1 * m2) # 2 x 2\n",
        "print(m1.mul(m2))\n",
        "\n",
        "# Shape of Matrix 1:  torch.Size([2, 2])\n",
        "# Shape of Matrix 2:  torch.Size([2, 1])\n",
        "# tensor([[1., 2.],\n",
        "#         [6., 8.]])\n",
        "# tensor([[1., 2.],\n",
        "#         [6., 8.]])"
      ],
      "metadata": {
        "id": "kZoSQPCYaiF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab8cc10-047a-4d48-c797-141ef8b969c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Matrix 1:  torch.Size([2, 2])\n",
            "Shape of Matrix 2:  torch.Size([2, 1])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 평균(Mean)\n",
        "\n",
        "`.mean()`"
      ],
      "metadata": {
        "id": "zJVfZJnvcvMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2차원인 행렬을 선언하여 .mean()을 사용해봅시다. 우선 2차원 행렬을 선언합니다."
      ],
      "metadata": {
        "id": "qcwFTzI-drfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(t)\n",
        "\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n",
        "\n",
        "print(t.mean())\n",
        "\n",
        "# tensor(2.5000)\n"
      ],
      "metadata": {
        "id": "CGyo8CyQdtf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6ceb80-6e3f-4984-e5ef-0aaeac85e6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor(2.5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4개의 원소의 평균인 2.5가 나왔습니다.\n",
        "\n",
        "이번에는 dim. 즉, 차원(dimension)을 인자로 주는 경우를 보겠습니다."
      ],
      "metadata": {
        "id": "SY-KrX5Td2fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.mean(dim=0))\n",
        "\n",
        "# tensor([2., 3.])\n"
      ],
      "metadata": {
        "id": "OTHeMQsWd5CZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb3af6b-5a93-478d-d528-edc6aa5643dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dim=0이라는 것은 첫번째 차원을 의미합니다.\n",
        "\n",
        "행렬에서 첫번째 차원은 '행'을 의미합니다.\n",
        "\n",
        "그리고 인자로 dim을 준다면 해당 차원을 제거한다는 의미가 됩니다.\n",
        "\n",
        "다시 말해 행렬에서 '열'만을 남기겠다는 의미가 됩니다.\n",
        "\n",
        "기존 행렬의 크기는 (2, 2)였지만 이를 수행하면 열의 차원만 보존되면서 (1, 2)가 됩니다.\n",
        "\n",
        "이는 (2,)와 같으며 벡터입니다.\n",
        "\n",
        "열의 차원을 보존하면서 평균을 구하면 아래와 같이 연산합니다."
      ],
      "metadata": {
        "id": "4z3anWgWd_Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 연산 과정\n",
        "# t.mean(dim=0)은 입력에서 첫번째 차원을 제거한다.\n",
        "\n",
        "# [[1., 2.],\n",
        "#  [3., 4.]]\n",
        "\n",
        "# 1과 3의 평균을 구하고, 2와 4의 평균을 구한다.\n",
        "# 결과 ==> [2., 3.]\n"
      ],
      "metadata": {
        "id": "hn3vauRGeE5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 인자로 dim=1을 주겠습니다. 이번에는 두번째 차원을 제거합니다. 즉, 열이 제거된 텐서가 되어야 합니다."
      ],
      "metadata": {
        "id": "iswYnuDLeHGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.mean(dim=1))\n",
        "\n",
        "# tensor([1.5000, 3.5000])\n"
      ],
      "metadata": {
        "id": "Dj7VpUdneJbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a4ae6c-9b6a-4d0e-d697-2bd9e03248d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.5000, 3.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "열의 차원이 제거되어야 하므로 (2, 2)의 크기에서 (2, 1)의 크기가 됩니다. 이번에는 1과 2의 평균을 구하고 3과 4의 평균을 구하게 됩니다. 그렇다면 결과는 아래와 같습니다."
      ],
      "metadata": {
        "id": "qPCd56lheN56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 연산 결과는 (2 × 1)\n",
        "# [1. 5]\n",
        "# [3. 5]\n"
      ],
      "metadata": {
        "id": "UtMpsit0ePPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만 (2 × 1)은 결국 1차원이므로 (1 × 2)와 같이 표현되면서 위와 같이 [1.5, 3.5]로 출력됩니다. 이번에는 dim=-1를 주는 경우를 보겠습니다. 이는 마지막 차원을 제거한다는 의미이고, 결국 열의 차원을 제거한다는 의미와 같습니다. 그러므로 위와 출력 결과가 같습니다."
      ],
      "metadata": {
        "id": "XBcCSCIseQ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.mean(dim=-1))\n",
        "\n",
        "# tensor([1.5000, 3.5000])\n"
      ],
      "metadata": {
        "id": "3RxTyuXieSMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb38e99d-61d7-427c-cb6a-4aa93cf688de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.5000, 3.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. 덧셈\n",
        "\n",
        "덧셈(Sum)은 평균(Mean)과 연산 방법이나 인자가 의미하는 바는 정확히 동일합니다."
      ],
      "metadata": {
        "id": "dYdPKSlTgx5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(t)\n",
        "\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n",
        "\n",
        "print(t.sum()) # 단순히 원소 전체의 덧셈을 수행\n",
        "print(t.sum(dim=0)) # 행을 제거\n",
        "print(t.sum(dim=1)) # 열을 제거\n",
        "print(t.sum(dim=-1)) # 열을 제거\n",
        "\n",
        "# tensor(10.)\n",
        "# tensor([4., 6.])\n",
        "# tensor([3., 7.])\n",
        "# tensor([3., 7.])"
      ],
      "metadata": {
        "id": "9eiJYsf4g2Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38938913-37e4-4157-94e4-545be0136e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor(10.)\n",
            "tensor([4., 6.])\n",
            "tensor([3., 7.])\n",
            "tensor([3., 7.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. 최대(Max)와 아그맥스(ArgMax)\n",
        "\n",
        "최대(Max)는 원소의 최대값을 리턴하고, 아그맥스(ArgMax)는 최대값을 가진 인덱스를 리턴합니다."
      ],
      "metadata": {
        "id": "hWxdRBjmhP-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(t)\n",
        "\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n",
        "\n",
        "print(t.max()) # Returns one value: max\n",
        "\n",
        "# tensor(4.)"
      ],
      "metadata": {
        "id": "IEJ4RMOShVsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397098be-bd14-4578-966e-e66c80e9d8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원소 중 최대값인 4를 리턴합니다.\n",
        "이번에는 인자로 dim=0을 주겠습니다. 첫번째 차원을 제거한다는 의미입니다."
      ],
      "metadata": {
        "id": "fWTFO6hhhc8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.max(dim=0)) # Returns two values: max and argmax\n",
        "\n",
        "# (tensor([3., 4.]), tensor([1, 1]))\n"
      ],
      "metadata": {
        "id": "elIqTVJ7heX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fa5d44-efed-43ce-a6a4-80c0a5583d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([3., 4.]),\n",
            "indices=tensor([1, 1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "행의 차원을 제거한다는 의미이므로 (1, 2) 텐서를 만듭니다. 결과는 [3, 4]입니다.\n",
        "\n",
        "그런데 [1, 1]이라는 값도 함께 리턴되었습니다. max에 dim 인자를 주면 argmax도 함께 리턴하는 특징 때문입니다.\n",
        "\n",
        "첫번째 열에서 3의 인덱스는 1이었습니다.\n",
        "\n",
        "두번째 열에서 4의 인덱스는 1이었습니다.\n",
        "\n",
        "그러므로 [1, 1]이 리턴됩니다. 어떤 의미인지는 아래 설명해봤습니다."
      ],
      "metadata": {
        "id": "u515YEb4hgxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1, 1]가 무슨 의미인지 봅시다. 기존 행렬을 다시 상기해봅시다.\n",
        "# [[1, 2],\n",
        "#  [3, 4]]\n",
        "# 첫번째 열에서 0번 인덱스는 1, 1번 인덱스는 3입니다.\n",
        "# 두번째 열에서 0번 인덱스는 2, 1번 인덱스는 4입니다.\n",
        "# 다시 말해 3과 4의 인덱스는 [1, 1]입니다.\n"
      ],
      "metadata": {
        "id": "Y2lSR4H7hmi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 두 개를 함께 리턴받는 것이 아니라 max 또는 argmax만 리턴받고 싶다면 다음과 같이 리턴값에도 인덱스를 부여하면 됩니다. 0번 인덱스를 사용하면 max 값만 받아올 수 있고, 1번 인덱스를 사용하면 argmax 값만 받아올 수 있습니다."
      ],
      "metadata": {
        "id": "KjIWWiCThriW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Max: ', t.max(dim=0)[0])\n",
        "print('Argmax: ', t.max(dim=0)[1])\n",
        "\n",
        "# Max:  tensor([3., 4.])\n",
        "# Argmax:  tensor([1, 1])\n"
      ],
      "metadata": {
        "id": "BrHR0EZ_hsr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741e5a05-e114-47bc-cd09-3002bd3337ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max:  tensor([3., 4.])\n",
            "Argmax:  tensor([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 dim=1로 인자를 주었을 때와 dim=-1로 인자를 주었을 때를 보겠습니다."
      ],
      "metadata": {
        "id": "ytL0Aubshvk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.max(dim=1))\n",
        "print(t.max(dim=-1))\n",
        "\n",
        "# (tensor([2., 4.]), tensor([1, 1]))\n",
        "# (tensor([2., 4.]), tensor([1, 1]))\n"
      ],
      "metadata": {
        "id": "rL59vMIZhwHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f780c77c-7c71-4543-d280-3708baf1e46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([2., 4.]),\n",
            "indices=tensor([1, 1]))\n",
            "torch.return_types.max(\n",
            "values=tensor([2., 4.]),\n",
            "indices=tensor([1, 1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추가 정보"
      ],
      "metadata": {
        "id": "V_2g3IAUIw4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 데이터사이언스 분야 한정으로 3차원 이상의 텐서는 그냥 다차원 행렬 또는 배열로 간주할 수 있습니다. 또한 주로 3차원 이상을 텐서라고 하긴 하지만, 1차원 벡터나 2차원인 행렬도 텐서라고 표현하기도 합니다. 같은 표현입니다. 벡터 = 1차원 텐서, 2차원 행렬 = 2차원 텐서, 그리고 3차원 텐서, 4차원 텐서, 5차원 텐서 등...\n",
        "\n",
        "- 훈련 데이터 하나의 크기를 256이라고 해봅시다. [3, 1, 2, 5, ...] 이런 숫자들의 나열이 256의 길이로 있다고 상상하면됩니다. 다시 말해 훈련 데이터 하나 = 벡터의 차원은 256입니다. 만약 이런 훈련 데이터의 개수가 3000개라고 한다면, 현재 전체 훈련 데이터의 크기는 3,000 × 256입니다. 행렬이니까 2D 텐서네요. 3,000개를 1개씩 꺼내서 처리하는 것도 가능하지만 컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리합니다. 3,000개에서 64개씩 꺼내서 처리한다고 한다면 이 때 batch size를 64라고 합니다. 그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 (batch size × dim) = 64 × 256입니다.\n",
        "\n",
        "- 텐서의 크기(shape)를 표현할 때는 ,(컴마)를 쓰기도 하고 ×(곱하기)를 쓰기도 합니다. 예를 들어 2행 3열의 2D 텐서를 표현할 때 (2, 3)라고 하기도 하고 (2 × 3)이라고 하기도 합니다. (5, )의 형식은 (1 × 5)를 의미합니다.\n",
        "\n",
        "- 훈련 데이터의 개수가 굉장히 많을 때, 컴퓨터가 한 번에 들고가서 처리할 양을 배치 크기(batch size)라고 합니다.\n",
        "\n",
        "[슬라이싱에 대한 다른 예제](https://wikidocs.net/13)\n"
      ],
      "metadata": {
        "id": "8GNRcj7Vh00R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경 (**매우 중요!**)\n",
        "\n",
        "파이토치 텐서의 뷰(View)는 넘파이에서의 리쉐이프(Reshape)와 같은 역할을 합니다. Reshape라는 이름에서 알 수 있듯이, 텐서의 크기(Shape)를 변경해주는 역할을 합니다."
      ],
      "metadata": {
        "id": "O-Xt4LjqQ3uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.array([[[0, 1, 2],\n",
        "               [3, 4, 5]],\n",
        "              [[6, 7, 8],\n",
        "               [9, 10, 11]]])\n",
        "ft = torch.FloatTensor(t)\n",
        "\n",
        "print(ft.shape)\n",
        "\n",
        "# torch.Size([2, 2, 3])\n",
        "# 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrlu7sZvV16d",
        "outputId": "08b0fbc9-2ad9-4d1f-c10c-708599a7db56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-f19a0d5fa445>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m t = np.array([[[0, 1, 2],\n\u001b[0m\u001b[1;32m      2\u001b[0m                [3, 4, 5]],\n\u001b[1;32m      3\u001b[0m               [[6, 7, 8],\n\u001b[1;32m      4\u001b[0m                [9, 10, 11]]])\n\u001b[1;32m      5\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3차원 텐서에서 2차원 텐서로 변경"
      ],
      "metadata": {
        "id": "TWWkJbdGWdc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
        "print(ft.view([-1, 3]).shape)\n",
        "\n",
        "# tensor([[ 0.,  1.,  2.],\n",
        "#         [ 3.,  4.,  5.],\n",
        "#         [ 6.,  7.,  8.],\n",
        "#         [ 9., 10., 11.]])\n",
        "# torch.Size([4, 3])\n"
      ],
      "metadata": {
        "id": "T1d1e4z9Wh8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "view([-1, 3])이 가지는 의미는 이와 같습니다.\n",
        "\n",
        "-1은 첫번째 차원은 사용자가 잘 모르겠으니 파이토치에 맡기겠다는 의미이고, 3은 두번째 차원의 길이는 3을 가지도록 하라는 의미입니다.\n",
        "\n",
        "다시 말해 현재 3차원 텐서를 2차원 텐서로 변경하되 (?, 3)의 크기로 변경하라는 의미입니다.\n",
        "\n",
        "결과적으로 (4, 3)의 크기를 가지는 텐서를 얻었습니다.\n",
        "\n",
        "내부적으로 크기 변환은 다음과 같이 이루어졌습니다. (2, 2, 3) -> (2 × 2, 3) -> (4, 3)\n",
        "\n",
        "규칙을 정리해봅시다.\n",
        "\n",
        "- view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 합니다.\n",
        "\n",
        "- 파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추합니다.\n",
        "\n",
        "변경 전 텐서의 원소의 수는 (2 × 2 × 3) = 12개였습니다.\n",
        "\n",
        "그리고 변경 후 텐서의 원소의 개수 또한 (4 × 3) = 12개였습니다."
      ],
      "metadata": {
        "id": "47ms5c7AWmEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3차원 텐서의 크기 변경\n",
        "\n",
        "이번에는 3차원 텐서에서 3차원 텐서로 차원은 유지하되, 크기(shape)를 바꾸는 작업을 해보겠습니다.\n",
        "\n",
        "view로 텐서의 크기를 변경하더라도 원소의 수는 유지되어야 한다고 언급한 바 있습니다.\n",
        "\n",
        "그렇다면 (2 × 2 × 3) 텐서를 (? × 1 × 3) 텐서로 변경하라고 하면 ?는 몇 차원인가요?\n",
        "\n",
        "(2 × 2 × 3) = (? × 1 × 3) = 12를 만족해야 하므로 ?는 4가 됩니다."
      ],
      "metadata": {
        "id": "VRi_1xDlaN2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.view([-1, 1, 3]))\n",
        "print(ft.view([-1, 1, 3]).shape)\n",
        "\n",
        "# tensor([[[ 0.,  1.,  2.]],\n",
        "\n",
        "#         [[ 3.,  4.,  5.]],\n",
        "\n",
        "#         [[ 6.,  7.,  8.]],\n",
        "\n",
        "#         [[ 9., 10., 11.]]])\n",
        "# torch.Size([4, 1, 3])\n"
      ],
      "metadata": {
        "id": "5T_I0uTIb-7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스퀴즈(Squeeze) - 1인 차원을 제거한다.\n",
        "\n",
        "스퀴즈는 차원이 1인 경우에는 해당 차원을 제거합니다."
      ],
      "metadata": {
        "id": "L7AIKWWUcFZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft = torch.FloatTensor([[0], [1], [2]])\n",
        "print(ft)\n",
        "print(ft.shape)\n",
        "\n",
        "# tensor([[0.],\n",
        "#         [1.],\n",
        "#         [2.]])\n",
        "# torch.Size([3, 1])\n",
        "\n",
        "print(ft.squeeze())\n",
        "print(ft.squeeze().shape)\n",
        "\n",
        "# tensor([0., 1., 2.])\n",
        "# torch.Size([3])\n"
      ],
      "metadata": {
        "id": "TUgnjujucgGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 언스퀴즈(Unsqueeze)\n",
        "\n",
        "특정 위치에 1인 차원을 추가한다."
      ],
      "metadata": {
        "id": "r9JrraRBdAqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft = torch.Tensor([0, 1, 2])\n",
        "print(ft.shape)\n",
        "\n",
        "# torch.Size([3])\n"
      ],
      "metadata": {
        "id": "xDMF6ArJdHxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재는 차원이 1개인 1차원 벡터입니다.\n",
        "\n",
        "여기에 첫번째 차원에 1인 차원을 추가해보겠습니다.\n",
        "\n",
        "첫번째 차원의 인덱스를 의미하는 숫자 0을 인자로 넣으면 첫번째 차원에 1인 차원이 추가됩니다."
      ],
      "metadata": {
        "id": "I208uv4TdTC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
        "print(ft.unsqueeze(0).shape)\n",
        "\n",
        "# tensor([[0., 1., 2.]])\n",
        "# torch.Size([1, 3])\n"
      ],
      "metadata": {
        "id": "PKK0-WcXdWDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 결과는 (3,)의 크기를 가졌던 1차원 벡터가 (1, 3)의 2차원 텐서로 변경된 것을 보여줍니다.\n",
        "\n",
        "방금 한 연산을 앞서 배운 view로도 구현 가능합니다.\n",
        "\n",
        "2차원으로 바꾸고 싶으면서 첫번째 차원은 1이기를 원한다면 view에서 (1, -1)을 인자로 사용하면됩니다."
      ],
      "metadata": {
        "id": "YrXtlxlqdiLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.view(1, -1))\n",
        "print(ft.view(1, -1).shape)\n",
        "\n",
        "# tensor([[0., 1., 2.]])\n",
        "# torch.Size([1, 3])\n"
      ],
      "metadata": {
        "id": "JRNhB3nbdoHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 결과는 unsqueeze와 view가 동일한 결과를 만든 것을 보여줍니다.\n",
        "\n",
        "이번에는 unsqueeze의 인자로 1을 넣어보겠습니다.\n",
        "\n",
        "인덱스는 0부터 시작하므로 이는 두번째 차원에 1을 추가하겠다는 것을 의미합니다.\n",
        "\n",
        "현재 크기는 (3,)이었으므로 두번째 차원에 1인 차원을 추가하면 (3, 1)의 크기를 가지게 됩니다."
      ],
      "metadata": {
        "id": "TpZCoC4TfW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.unsqueeze(1))\n",
        "print(ft.unsqueeze(1).shape)\n",
        "\n",
        "# tensor([[0.],\n",
        "#         [1.],\n",
        "#         [2.]])\n",
        "# torch.Size([3, 1])\n"
      ],
      "metadata": {
        "id": "2zHIz-VjfrgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 unsqueeze의 인자로 -1을 넣어보겠습니다.\n",
        "\n",
        "-1은 인덱스 상으로 마지막 차원을 의미합니다.\n",
        "\n",
        "현재 크기는 (3,)이었으므로 마지막 차원에 1인 차원을 추가하면 (3, 1)의 크기를 가지게 됩니다.\n",
        "\n",
        "다시 말해 현재 텐서의 경우에는 1을 넣은 경우와 -1을 넣은 경우가 결과가 동일합니다."
      ],
      "metadata": {
        "id": "oxpaufJAfwHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.unsqueeze(-1))\n",
        "print(ft.unsqueeze(-1).shape)\n",
        "\n",
        "# tensor([[0.],\n",
        "#         [1.],\n",
        "#         [2.]])\n",
        "# torch.Size([3, 1])\n"
      ],
      "metadata": {
        "id": "edxQRlRshTFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** view(), squeeze(), unsqueeze()는 텐서의 원소 수를 그대로 유지하면서 모양과 차원을 조절한다.**"
      ],
      "metadata": {
        "id": "tvU59D04hYG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 타입 캐스팅 (Type Casting)\n",
        "\n",
        "텐서에는 자료형이라는 것이 있습니다.\n",
        "\n",
        "각 데이터형별로 정의되어져 있는데, 예를 들어 32비트의 부동 소수점은 torch.FloatTensor를, 64비트의 부호 있는 정수는 torch.LongTensor를 사용합니다.\n",
        "\n",
        "GPU 연산을 위한 자료형도 있습니다. 예를 들어 torch.cuda.FloatTensor가 그 예입니다.\n",
        "\n",
        "그리고 이 자료형을 변환하는 것을 타입 캐스팅이라고 합니다."
      ],
      "metadata": {
        "id": "wZBNkcTAhe19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lt = torch.LongTensor([1, 2, 3, 4])\n",
        "\n",
        "print(lt.float())\n",
        "\n",
        "# tensor([1., 2., 3., 4.])\n"
      ],
      "metadata": {
        "id": "jd8wOgB4htrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bt = torch.ByteTensor([True, False, False, True])\n",
        "print(bt)\n",
        "\n",
        "# tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
        "\n",
        "print(bt.long())\n",
        "print(bt.float())\n",
        "\n",
        "# tensor([1, 0, 0, 1])\n",
        "# tensor([1., 0., 0., 1.])\n"
      ],
      "metadata": {
        "id": "zwdIn0UCh1tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 연결하기(concatenate)\n",
        "\n",
        "두 텐서를 연결하는 방법에 대해서 알아보겠습니다."
      ],
      "metadata": {
        "id": "yx17uQaflqWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "y = torch.FloatTensor([[5, 6], [7, 8]])\n"
      ],
      "metadata": {
        "id": "dFtuOzxNlyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 두 텐서를 torch.cat([ ])를 통해 연결해보겠습니다.\n",
        "\n",
        "그런데 연결 방법은 한 가지만 있는 것이 아닙니다.\n",
        "\n",
        "torch.cat은 어느 차원을 늘릴 것인지를 인자로 줄 수 있습니다.\n",
        "\n",
        "예를 들어 dim=0은 첫번째 차원을 늘리라는 의미를 담고있습니다."
      ],
      "metadata": {
        "id": "yDOi8j_Xl1xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cat([x, y], dim=0))\n",
        "\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.],\n",
        "#         [5., 6.],\n",
        "#         [7., 8.]])\n"
      ],
      "metadata": {
        "id": "PJ6LuIa8l7je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cat([x, y], dim=1))\n",
        "\n",
        "# tensor([[1., 2., 5., 6.],\n",
        "        # [3., 4., 7., 8.]])\n"
      ],
      "metadata": {
        "id": "fuYZVwMiqKyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dim=1을 인자로 했더니 두 개의 (2 × 2) 텐서가 (2 × 4) 텐서가 된 것을 볼 수 있습니다.\n",
        "\n",
        "- 딥 러닝에서는 주로 모델의 입력 또는 중간 연산에서 두 개의 텐서를 연결하는 경우가 많습니다. 두 텐서를 연결해서 입력으로 사용하는 것은 두 가지의 정보를 모두 사용한다는 의미를 가지고 있습니다."
      ],
      "metadata": {
        "id": "svq-6dLBqRpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스택킹(Stacking)\n",
        "\n",
        "연결(concatenate)을 하는 또 다른 방법으로 스택킹(Stacking)이 있습니다.\n",
        "\n",
        "스택킹은 영어로 쌓는다는 의미입니다. 때로는 연결을 하는 것보다 스택킹이 더 편리할 때가 있는데, 이는 스택킹이 많은 연산을 포함하고 있기 때문입니다."
      ],
      "metadata": {
        "id": "DuGaG7RauSvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([1, 4])\n",
        "y = torch.FloatTensor([2, 5])\n",
        "z = torch.FloatTensor([3, 6])\n",
        "\n",
        "print(torch.stack([x, y, z]))\n",
        "\n",
        "# tensor([[1., 4.],\n",
        "#         [2., 5.],\n",
        "#         [3., 6.]])\n"
      ],
      "metadata": {
        "id": "l9kVhyHQwkOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 결과는 3개의 벡터가 순차적으로 쌓여 (3 × 2) 텐서가 된 것을 보여줍니다.\n",
        "\n",
        "스택킹은 사실 많은 연산을 한 번에 축약하고 있습니다. 예를 들어 위 작업은 아래의 코드와 동일한 작업입니다."
      ],
      "metadata": {
        "id": "ha-GdrhmwpHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))\n",
        "\n",
        "# tensor([[1., 4.],\n",
        "#         [2., 5.],\n",
        "#         [3., 6.]])\n"
      ],
      "metadata": {
        "id": "vqTrjQ-DwtZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x, y, z는 기존에는 전부 (2,)의 크기를 가졌습니다. 그런데 .unsqueeze(0)을 하므로서 3개의 벡터는 전부 (1, 2)의 크기의 2차원 텐서로 변경됩니다. 여기에 연결(concatenate)를 의미하는 cat을 사용하면 (3 x 2) 텐서가 됩니다."
      ],
      "metadata": {
        "id": "C2uOEWRgwu84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서는 torch.stack([x, y, z])라는 한 번의 커맨드로 수행했지만, 연결(concatenate)로 이를 구현하려고 했더니 꽤 복잡해졌습니다.\n",
        "\n",
        "스택킹에 추가적으로 dim을 인자로 줄 수도 있습니다. 이번에는 dim=1 인자를 주겠습니다.\n",
        "이는 두번째 차원이 증가하도록 쌓으라는 의미로 해석할 수 있습니다."
      ],
      "metadata": {
        "id": "4EBqGE8vw0YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.stack([x, y, z], dim=1))\n",
        "\n",
        "# tensor([[1., 2., 3.],\n",
        "#         [4., 5., 6.]])\n"
      ],
      "metadata": {
        "id": "dxYQI-7hw2Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 결과는 두번째 차원이 증가하도록 스택킹이 된 결과를 보여줍니다.\n",
        "결과적으로 (2 × 3) 텐서가 됩니다."
      ],
      "metadata": {
        "id": "M2hJGU4kw45s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ones_like와 zeros_like\n",
        "\n",
        "0으로 채워진 텐서와 1로 채워진 텐서"
      ],
      "metadata": {
        "id": "jz3elxEeyLNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
        "print(x)\n",
        "\n",
        "# tensor([[0., 1., 2.],\n",
        "#         [2., 1., 0.]])\n",
        "\n",
        "print(torch.ones_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기\n",
        "\n",
        "# tensor([[1., 1., 1.],\n",
        "#         [1., 1., 1.]])\n",
        "\n",
        "print(torch.zeros_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 0으로 채우기\n",
        "\n",
        "# tensor([[0., 0., 0.],\n",
        "#         [0., 0., 0.]])\n"
      ],
      "metadata": {
        "id": "WdMIkawsyQ8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-place Operation (덮어쓰기 연산)\n"
      ],
      "metadata": {
        "id": "LLVGRGwC11wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력\n",
        "print(x) # 기존의 값 출력\n",
        "\n",
        "# tensor([[2., 4.],\n",
        "#         [6., 8.]])\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n"
      ],
      "metadata": {
        "id": "QpSNXdL018m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 출력은 곱하기 2가 수행된 결과를 보여주고, 두번째 출력은 기존의 값이 그대로 출력된 것을 확인할 수 있습니다. 곱하기 2를 수행했지만 이를 x에다가 다시 저장하지 않았으니, 곱하기 연산을 하더라도 기존의 값 x는 변하지 않는 것이 당연합니다.\n",
        "\n",
        "그런데 연산 뒤에 _를 붙이면 기존의 값을 덮어쓰기 합니다."
      ],
      "metadata": {
        "id": "4n4wL3C62BOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력\n",
        "print(x) # 기존의 값 출력\n",
        "\n",
        "# tensor([[2., 4.],\n",
        "#         [6., 8.]])\n",
        "# tensor([[2., 4.],\n",
        "#         [6., 8.]])\n"
      ],
      "metadata": {
        "id": "asdkhTCH2CbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
